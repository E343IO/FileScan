# Thought process
  
Initially, I thought of having four main services: FileScanApp, AntiVirusScanner, MetaDataExtractor, Aggregator. The FileScanApp was responsible for accepting the file and user email and then uploading that file to AWS S3. Once the file has been uploaded to the S3 bucket, the FileScanApp will have the link to the uploaded file. This link would be then pushed to the message queue alongwith the user email id. As soon as the state of the message queue would change, the MetaDataExtractor and AntiVirusScanner microservices which are subsribed to the message queue would be notified and it would pull the changes from the queue. Both the microservices would fetch the file from the S3 bucket and process them. Once they both finish processing, they would put the results back to the queue to some common topic, to which the Aggregator would have subscribed to. Finally, the aggregator would send the email/message to the user.

## Refactoring 1.1 (Issue: Separation of concern)

The above approach was okay for a MVP but it has some major issues. Firstly, the **separation of concerns**. The FileScanApp had multiple roles including accpeting inputs from the user, uploading the file to AWS S3 and then pushing the message to the queue. Clearly, the coupling was too tight and prone to issues in the future. So, the first thing to do was to separate out the duties. The FileScanApp is now merely responsible for accepting the input from the user. I introduced a new service StorageManager which would be responsible for uploading the file to a file storage not limited to AWS S3. It is a generic service that can have as many storage options as needed. All it does it accpets a file and storage provider, upload it to the cloud and returns the public URL of the file. Similarly, the Aggregator service should be responsbile only for combining the results from the multiple microservices and that's it. So, I created another microservice that receives the input from the Aggregator. Then what?

## Refactoring 1.2 (Issue: Inconsistent result after merging)

The problem with the above approach was with the Aggregator. First of all, the **merging logic for Aggregator** was sort of a challenge. There is a possibility of inconsistent ordering of output in the merged output since the rate of processing of the services may vary. So, although the AntiVirusScan and MetaDataExtractor services are independent of each other, I would rather process the file serially so as to avoid the merging logic and therefore eliminating the Aggregator. Basically, I'd pass the file serially to the services alongwith the email and a placeholder for the output and each service would keep appending the output to the placeholder. Ultimately, the last service would forward the result to the Notifier service.

## Refactoring 1.3 (Spaming and Redundant processing)

Another major change I'd have to introduce would be either rate limiter for the service or enforce login.
The current architecture is vulnerable to spam emails. Attacker can user spam emails to overwhelm the service. This makes it manadatory to either enforce login and implementing rate limiter. By enforcing login, we can ensure that we restrict the number of scans per day/hour/minute and also add additional features for future. Another issue with the above approach is processing the same file again and again. Lets say its year 2020 and the song Brown Munde is trending across India. A huge number of users might use the service to scan the song for virus. Since the current architecture does not have caching mechanism, our app will compute the result for the same file multiple times, thereby wasting valuable resources. Hence, we need to introcude caching. But where? The best place to have a cache is on the FileScanApp. If we have a cache at this service, we can ensure that we don't store the same file repeatedly in S3 bucket (saving storage space) and also do not invoke the services (saving compute power). We can store (filehash, result) in the cache at the FileScanApp and it would ensure that as long as the same file is being tested, we return the result from the cache. With caching in place, another question that arises is the Cache eviction policy. What could possibly be the best eviction policy? - LFU? Maybe? This would make sure that the most popular files remain in our cache all the time and saves us the storage and compute power. LFU would work most of the times except a few. Lets imagine a scenario: lets say its year 2013 and Gangnam style.mp3 is gaining popularity across the globe. Millions of people downloaded that file and used our service to scan for viruses. This went on for a month. This file was on the top of our LFU cache since it was accessed most frequently. Now, after a month or two, suddenly the number of downloads for this file dropped and in no time the count dropped down from order of millions to hundreds. But with LFU policy, the file will continue to be on top of our cache for a very long time. Hence the problem. How about LRU? Lets consider the above case but use LRU eviction this time. This time, with everytime a file other than Gangnam style.mp3 would be used, Gangnam style.mp3 would lose its strength in the LRU cache and eventually as its usage would decrease, it would become eligible for evition from the cache. Therefore, LRU cache would be a perfect fit here.

## Implementation

**Placement of the redis server**
The first service I started implementing was the FileScanApp. The first question to address while implemeting the FileScanApp was to decide the placement of the Redis Server for caching. It certainly needed to be between the FileScanApp and the StorageManager. Secondly, should there be a separate cache server for each microservice or a common for all.

Deploying Docker image to run Redis Server.